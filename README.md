Codes for my thesis.  
The original GPT model code had a coding error which lead to the model not shuffling the data correctly, and thus there was a data leakage. It's been fixed in the second code.  
The reason to keep the original code here, is because in the thesis some of the analysis uses the results from it, thus the reader can still see the original data, if they so choose.  
The optimal order is to go trough the GPT notebook first, as that has more information and explanations about the decoder and how transformer models work. The encoder-decoder notebook emits these, as the assumption is that the reader starts with the GPT notebook.
